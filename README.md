# cloud_cost_optimization
This project will detect which of the process is using the unnecessary resources 

ABSTRACT

With the rapid growth of large-scale distributed computing systems, efficient resource management has become a critical challenge for modern data centers and cloud platforms. In environments such as cluster computing frameworks and big data processing systems, multiple jobs and tasks execute concurrently while competing for limited resources such as CPU, memory, disk I/O, and network bandwidth. Inefficient resource utilization not only degrades system performance but also increases operational costs and leads to job failures, delays, or forced terminations. This project focuses on identifying resource inefficiencies at the job and task level, detecting processes that consume excessive resources beyond their allocated limits, and predicting the final execution state of each process.

The primary objective of this project is to analyze system-level job execution data to determine which job tasks are using resources excessively without actual requirement and which processes are requesting more resources than those initially assigned to them. In addition, the project aims to build predictive models that can accurately forecast the future state of a process, such as finished, failed, submitted, scheduled, evicted, killed, or lost. By achieving these objectives, the system can help administrators proactively manage resources, reduce system failures, and improve overall cluster efficiency.

The dataset provided for this project contains detailed logs of job and task executions, including resource requests, actual resource consumption, execution duration, scheduling information, and final task states. These logs represent real-world behavior of large-scale distributed systems, making the dataset suitable for both analytical and predictive modeling tasks. Data preprocessing plays a crucial role in this project, as raw system logs often contain missing values, noisy records, redundant features, and skewed distributions. Therefore, extensive data cleaning, transformation, normalization, and feature engineering techniques are applied to prepare the dataset for effective analysis.

Exploratory Data Analysis (EDA) is conducted to understand resource usage patterns across different job types and execution states. Statistical analysis and visualizations are used to identify trends such as over-provisioning, underutilization, frequent resource contention, and correlations between resource consumption and job outcomes. This analysis helps in distinguishing between tasks that genuinely require high resources and those that inefficiently consume system capacity without proportional workload demands.

To detect resource misuse, rule-based thresholds and anomaly detection techniques are applied. These methods compare requested resources with actual utilization to flag tasks that consistently overuse or underuse allocated resources. Processes that request significantly more resources than assigned are identified as potential sources of system imbalance and performance degradation. Such insights can be used to optimize scheduling policies and resource allocation strategies.

For predictive analysis, machine learning models are developed to classify and predict the final state of each process. Features such as CPU usage, memory consumption, execution time, scheduling delay, and historical behavior are used as input variables. Classification algorithms are trained and evaluated to predict whether a process will successfully finish or end in states such as failure, eviction, kill, or loss. The predictive component of the system enables early intervention, allowing corrective actions to be taken before resource wastage or job failure occurs.

The technology stack for this project is carefully chosen to support scalable data processing, model development, and visualization. Python is used as the primary programming language due to its strong ecosystem for data analysis and machine learning. Machine learning and deep learning libraries are utilized to build and evaluate predictive models. SQL is used for structured data storage and efficient querying of processed datasets. Big data tools such as Hadoop and PySpark are employed to handle large volumes of log data and perform distributed data processing. Workflow orchestration and automation are managed using Airflow, while version control and project management are handled through GitHub and Jira.

For user interaction and result presentation, a lightweight web-based interface is developed using Flask along with HTML and CSS, with optional integration of React for enhanced user experience. Cloud deployment is supported using AWS, enabling scalability and real-world applicability of the solution. Analytical dashboards and visual reports are created using Tableau to provide intuitive insights into resource utilization patterns, anomalies, and prediction results.

The project is designed to be completed within a duration of one month, following a structured workflow that includes data understanding, preprocessing, analysis, model development, evaluation, and deployment. The final outcome of this project is an intelligent system capable of monitoring resource usage, detecting inefficiencies, and predicting process states in distributed computing environments. Such a system can significantly enhance operational efficiency, reduce system failures, and support data-driven decision-making in modern cloud and cluster-based infrastructures.

